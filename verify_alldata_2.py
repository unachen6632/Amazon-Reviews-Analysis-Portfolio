# filename: validate_all.py
# ä½œç”¨ï¼š
# - ç¸½é©—è­‰è…³æœ¬ï¼Œç”¨æ–¼æª¢æŸ¥ 1_clean_data.py (core_analysis.py) çš„æ‰€æœ‰ 6 å€‹è¼¸å‡ºæª”æ¡ˆã€‚
# - é©—è­‰ (1) æª”æ¡ˆæ˜¯å¦å­˜åœ¨ (2) çµæ§‹/è¡¨é ­æ˜¯å¦æ­£ç¢º (3) å¤§å‹æª”æ¡ˆçš„è³‡æ–™å®Œæ•´æ€§ (è¡Œæ•¸)ã€‚
# - (v2 æ›´æ–°ï¼šé…åˆæœ€æ–°çš„ä¸­æ–‡è³‡æ–™å­—å…¸å®šç¾©)

import sys
import json
import csv
import pandas as pd
from pathlib import Path

# --- 1. åŸºæœ¬è¨­å®šï¼ˆå¿…é ˆèˆ‡ 1_clean_data.py ä¿æŒä¸€è‡´ï¼‰---

# é æœŸè·¯å¾‘
try:
    BASE_DIR = Path(__file__).resolve().parent
except NameError:
    BASE_DIR = Path.cwd()

DATA_PROCESSED = BASE_DIR / "data_processed"
ENCODING = "utf-8-sig" # åŒ¹é… ETL è¼¸å‡º

# é æœŸæª”æ¡ˆåç¨± (ä¾†è‡ª meta.json)
PATH_FILTERED = DATA_PROCESSED / "amazon_reviews_2010_2015_filtered.csv"
PATH_CLEAN = DATA_PROCESSED / "reviews_clean.csv"
PATH_AGG_ALL = DATA_PROCESSED / "monthly_agg_all.csv"
PATH_AGG_VER = DATA_PROCESSED / "monthly_agg_verified.csv"
PATH_DICT = DATA_PROCESSED / "data_dictionary.json"
PATH_META = DATA_PROCESSED / "clean_meta.json"


# --- 2. é æœŸçµæ§‹ï¼ˆä¾†è‡ª 1_clean_data.py çš„ç¨‹å¼é‚è¼¯ï¼‰---

# ã€ä¿®æ­£ã€‘é€™è£¡æ›´æ–°ç‚ºæ‚¨æŒ‡å®šçš„æœ€æ–°ç°¡åŒ–ç‰ˆç¿»è­¯
EXPECTED_DATA_DICT = {
    # åŸºç¤æ¬„ä½ (æ ¹æ“šæ‚¨æœ€æ–°çš„ç¿»è­¯è¦æ±‚)
    "marketplace": "å¸‚å ´",
    "customer_id": "é¡§å®¢ç·¨è™Ÿ",
    "review_id": "è©•è«–ç·¨è™Ÿ",
    "product_id": "å•†å“ç·¨è™Ÿ",
    "product_parent": "ç”¢å“ä¸»ç·¨è™Ÿ",
    "product_title": "ç”¢å“æ¨™é¡Œ",
    "product_category": "ç”¢å“é¡åˆ¥",
    "star_rating": "æ˜Ÿç­‰",
    "helpful_votes": "æœ‰å¹«åŠ©çš„ç¥¨æ•¸",
    "total_votes": "ç¸½ç¥¨æ•¸",
    "vine": "Vine è¨ˆç•«æ¨™è¨˜",
    "verified_purchase": "å·²é©—è­‰è³¼è²·",
    "review_headline": "è©•è«–æ¨™é¡Œ",
    "review_body": "è©•è«–å…§å®¹",
    "review_date": "è©•è«–æ—¥æœŸ",
    
    # åŸºç¤è¡ç”Ÿæ¬„ä½ (åŸæœ¬çš„å®šç¾©)
    "review_year": "è©•è«–å¹´ä»½",
    "review_month": "è©•è«–æœˆä»½",
    
    # é€²éšè¡ç”Ÿæ¬„ä½ (åŸæœ¬çš„å®šç¾©)
    "is_verified": "æ˜¯å¦å·²é©—è­‰è³¼è²·",
    "is_vine": "æ˜¯å¦ Vine",
    "helpful_ratio": "æœ‰å¹«åŠ©ç¥¨æ¯”ä¾‹",
    "weight": "è©•åƒ¹æ¬Šé‡(1+log1p(helpful))ï¼Œå°é ‚8",
    "review_body_clean": "æ¸…ç†å¾Œè©•è«–æ–‡æœ¬",
    "sentiment_label": "ç²—æƒ…ç·’æ¨™ç±¤(æ˜Ÿç­‰â†’Neg/Neu/Pos)",
    "cs_issue": "å®¢æœå•é¡Œ",
    "delivery_issue": "ç‰©æµå•é¡Œ",
    "return_issue": "é€€æ›/é€€æ¬¾å•é¡Œ",
    "fulfillment_issue": "å‡ºè²¨/å“é …å•é¡Œ",
    "service_any": "ä»»ä¸€æœå‹™å•é¡Œ",
    "is_neg": "æ˜¯å¦è² è©•(<=2â˜…)",
    "is_service_neg": "æœå‹™ç›¸é—œè² è©•"
}

# ä¾†è‡ª 1_clean_data.py -> OnlineMonthAgg._to_frame å‡½å¼
EXPECTED_COLS_AGG = [
    "review_year","review_month","n","avg_star","neg_rate",
    "service_neg_rate","w_neg_rate"
]

# ä¾†è‡ª 1_clean_data.py -> base_cols è®Šæ•¸
EXPECTED_COLS_FILTERED = [
    "marketplace","customer_id","review_id","product_id","product_parent",
    "product_title","product_category","star_rating","helpful_votes","total_votes",
    "verified_purchase","review_headline","review_body","review_date",
    "review_year","review_month"
]

# ä¾†è‡ª 1_clean_data.py -> clean_cols è®Šæ•¸
EXPECTED_COLS_CLEAN = [
    "marketplace","customer_id","review_id","product_id","product_parent",
    "product_title","product_category","star_rating","helpful_votes","total_votes",
    "verified_purchase","review_headline","review_body","review_date",
    "review_year","review_month",
    "is_verified","is_vine","helpful_ratio","weight","review_body_clean",
    "sentiment_label","cs_issue","delivery_issue","return_issue",
    "fulfillment_issue","service_any","is_neg","is_service_neg"
]

# --- 3. é©—è­‰è¼”åŠ©å‡½å¼ ---

def check_path(p: Path) -> bool:
    """æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨"""
    if not p.exists():
        print(f"âŒ [å¤±æ•—] æª”æ¡ˆä¸å­˜åœ¨: {p.name}")
        return False
    return True

def validate_json(filepath: Path, expected_content: dict = None) -> bool:
    """é©—è­‰ JSON æª”æ¡ˆï¼š(1) å­˜åœ¨ (2) å¯è®€ (3) [å¯é¸] å…§å®¹åŒ¹é…"""
    print(f"ğŸ”¬ é©—è­‰ (JSON): {filepath.name}")
    if not check_path(filepath): return False
    
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        if expected_content:
            # ç‚ºäº†å®¹éŒ¯ï¼Œæˆ‘å€‘å¯ä»¥æª¢æŸ¥ key æ˜¯å¦éƒ½å­˜åœ¨ï¼Œæˆ–è€…å®Œå…¨åŒ¹é…
            # é€™è£¡ä½¿ç”¨å®Œå…¨åŒ¹é…
            if data == expected_content:
                print(f"âœ… [æˆåŠŸ] {filepath.name} å…§å®¹å®Œå…¨åŒ¹é…ã€‚")
                return True
            else:
                print(f"âŒ [å¤±æ•—] {filepath.name} å…§å®¹ä¸åŒ¹é…ï¼")
                # é¡¯ç¤ºå·®ç•° (Debug ç”¨)
                diff_keys = [k for k in expected_content if k not in data or data[k] != expected_content[k]]
                if diff_keys:
                    print(f"   - å·®ç•°æ¬„ä½ç¯„ä¾‹ (å‰3å€‹): {diff_keys[:3]}")
                    print(f"   - é æœŸ: {expected_content[diff_keys[0]]}")
                    print(f"   - å¯¦éš›: {data.get(diff_keys[0], 'ä¸å­˜åœ¨')}")
                return False
        
        # å¦‚æœåªæ˜¯æª¢æŸ¥å¯è®€æ€§
        print(f"âœ… [æˆåŠŸ] {filepath.name} æ ¼å¼æ­£ç¢ºã€‚")
        return True
        
    except Exception as e:
        print(f"âŒ [å¤±æ•—] {filepath.name} è®€å–æˆ–è§£æå¤±æ•—: {e}")
        return False

def validate_csv_header(filepath: Path, expected_cols: list) -> bool:
    """(é©ç”¨å°å‹ CSV) é©—è­‰è¡¨é ­"""
    print(f"ğŸ”¬ é©—è­‰ (CSV Header): {filepath.name}")
    if not check_path(filepath): return False
    
    try:
        with open(filepath, "r", encoding=ENCODING, newline='') as f:
            header = next(csv.reader(f))
        
        if header == expected_cols:
            print(f"âœ… [æˆåŠŸ] {filepath.name} è¡¨é ­ (æ¬„ä½) OKã€‚")
            return True
        else:
            print(f"âŒ [å¤±æ•—] {filepath.name} è¡¨é ­ (æ¬„ä½) ä¸åŒ¹é…ï¼")
            print(f"   - é æœŸ: {expected_cols}")
            print(f"   - å¯¦éš›: {header}")
            return False
            
    except Exception as e:
        print(f"âŒ [å¤±æ•—] {filepath.name} è®€å–å¤±æ•—: {e}")
        return False

def validate_large_csv(filepath: Path, expected_cols: list, expected_rows: int) -> bool:
    """(é©ç”¨å¤§å‹ CSV) ä¸²æµé©—è­‰è¡¨é ­å’Œç¸½è¡Œæ•¸"""
    print(f"ğŸ”¬ é©—è­‰ (Large CSV): {filepath.name}")
    if not check_path(filepath): return False

    try:
        # 1. é©—è­‰è¡¨é ­
        header_df = pd.read_csv(filepath, encoding=ENCODING, nrows=0, low_memory=False)
        header = list(header_df.columns)
            
        if header != expected_cols:
            print(f"âŒ [å¤±æ•—] {filepath.name} è¡¨é ­ (æ¬„ä½) ä¸åŒ¹é…ï¼")
            print(f"   - é æœŸ: {expected_cols}")
            print(f"   - å¯¦éš›: {header}")
            return False
        
        # 2. é©—è­‰è¡Œæ•¸ (Pandas ä¸²æµè¨ˆæ•¸)
        row_count = 0
        reader_col = expected_cols[0] # å„ªåŒ–ï¼šåªè®€ä¸€æ¬„
        
        chunk_iter = pd.read_csv(
            filepath, 
            encoding=ENCODING, 
            chunksize=200_000,
            low_memory=False,
            usecols=[reader_col]
        )
        
        for chunk in chunk_iter:
            row_count += len(chunk)
        
        if row_count != expected_rows:
            print(f"âŒ [å¤±æ•—] {filepath.name} ç¸½è¡Œæ•¸ä¸åŒ¹é…ï¼")
            print(f"   - é æœŸ (ä¾†è‡ª meta.json): {expected_rows:,} è¡Œ")
            print(f"   - å¯¦éš› (æª”æ¡ˆå…§): {row_count:,} è¡Œ")
            return False
            
        print(f"âœ… [æˆåŠŸ] {filepath.name} è¡¨é ­ (æ¬„ä½) OKã€‚")
        print(f"âœ… [æˆåŠŸ] {filepath.name} ç¸½è¡Œæ•¸ OK ({row_count:,} è¡Œ)ã€‚")
        return True
            
    except Exception as e:
        print(f"âŒ [å¤±æ•—] {filepath.name} é©—è­‰æ™‚ç™¼ç”Ÿç¨‹å¼éŒ¯èª¤: {e}")
        return False

# --- 4. ä¸»åŸ·è¡Œæµç¨‹ ---

def main():
    print("=" * 60)
    print("é–‹å§‹åŸ·è¡Œ ETL (1_clean_data.py) ç¸½è¼¸å‡ºé©—è­‰...")
    print(f"è³‡æ–™å¤¾: {DATA_PROCESSED}")
    print("=" * 60)
    
    results = {}
    
    # æ­¥é©Ÿ 0: è®€å– Meta - é€™æ˜¯å¾ŒçºŒé©—è­‰çš„åŸºç¤
    print(f"ğŸ”¬ é©—è­‰ (JSON): {PATH_META.name}")
    if not check_path(PATH_META):
        print("âŒ [è‡´å‘½éŒ¯èª¤] meta.json ä¸å­˜åœ¨ã€‚ç„¡æ³•ç¹¼çºŒé©—è­‰ã€‚")
        sys.exit()
    try:
        with open(PATH_META, "r", encoding="utf-8") as f:
            meta = json.load(f)
        EXPECTED_ROWS = meta["rows_kept"]
        print(f"âœ… [æˆåŠŸ] {PATH_META.name} è®€å–æˆåŠŸã€‚")
        print(f"   ...é æœŸè³‡æ–™è¡Œæ•¸: {EXPECTED_ROWS:,} è¡Œ")
        results["meta"] = True
    except Exception as e:
        print(f"âŒ [è‡´å‘½éŒ¯èª¤] {PATH_META.name} è®€å–æˆ–è§£æå¤±æ•—: {e}")
        sys.exit()

    print("-" * 60)
    
    # æ­¥é©Ÿ 1: é©—è­‰ Data Dictionary
    results["dict"] = validate_json(PATH_DICT, EXPECTED_DATA_DICT)
    
    print("-" * 60)
    
    # æ­¥é©Ÿ 2: é©—è­‰æœˆå½™ç¸½è¡¨ (å°å‹ CSV)
    results["agg_all"] = validate_csv_header(PATH_AGG_ALL, EXPECTED_COLS_AGG)
    results["agg_ver"] = validate_csv_header(PATH_AGG_VER, EXPECTED_COLS_AGG)

    print("-" * 60)

    # æ­¥é©Ÿ 3: é©—è­‰å¤§å‹è³‡æ–™æª”æ¡ˆ (å¤§å‹ CSV)
    results["filtered"] = validate_large_csv(PATH_FILTERED, EXPECTED_COLS_FILTERED, EXPECTED_ROWS)
    results["clean"] = validate_large_csv(PATH_CLEAN, EXPECTED_COLS_CLEAN, EXPECTED_ROWS)

    print("=" * 60)
    
    # ç¸½çµ
    total_checks = len(results)
    success_checks = sum(results.values())
    
    if success_checks == total_checks:
        print(f"âœ…âœ…âœ… ç¸½çµè«–ï¼šå…¨éƒ¨ {total_checks} é …æª¢æŸ¥å‡å·²é€šéï¼")
        print("ETL è¼¸å‡ºè³‡æ–™å·² 100% é©—è­‰æ­£ç¢ºã€‚")
    else:
        print(f"âŒâŒâŒ ç¸½çµè«–ï¼š{total_checks} é …æª¢æŸ¥ä¸­ï¼Œæœ‰ {total_checks - success_checks} é …å¤±æ•—ã€‚")
        print("è«‹æª¢æŸ¥ä¸Šæ–¹çš„ [å¤±æ•—] è¨Šæ¯ã€‚")
    
    print("=" * 60)

if __name__ == "__main__":
    main()